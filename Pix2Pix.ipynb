{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "207e05ac",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-11-04T20:36:30.529567Z",
          "start_time": "2023-11-04T20:36:30.510549Z"
        },
        "id": "207e05ac",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a8c86002-fa2a-4174-dd38-acdd23e8fddc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torchmetrics\n",
            "  Downloading torchmetrics-1.2.0-py3-none-any.whl (805 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m805.2/805.2 kB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>1.20.0 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (1.23.5)\n",
            "Requirement already satisfied: torch>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (2.1.0+cu118)\n",
            "Collecting lightning-utilities>=0.8.0 (from torchmetrics)\n",
            "  Downloading lightning_utilities-0.10.0-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: packaging>=17.1 in /usr/local/lib/python3.10/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (23.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (67.7.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (4.5.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->torchmetrics) (3.13.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->torchmetrics) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->torchmetrics) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->torchmetrics) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->torchmetrics) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->torchmetrics) (2.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.8.1->torchmetrics) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.8.1->torchmetrics) (1.3.0)\n",
            "Installing collected packages: lightning-utilities, torchmetrics\n",
            "Successfully installed lightning-utilities-0.10.0 torchmetrics-1.2.0\n"
          ]
        }
      ],
      "source": [
        "pip install torchmetrics"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from PIL import Image, ImageColor\n",
        "from google.colab import drive\n",
        "import skimage.io as io\n",
        "import numpy as np\n",
        "from scipy import ndimage\n",
        "from torchmetrics.classification import Dice"
      ],
      "metadata": {
        "id": "SPZaLrQVAZV4"
      },
      "id": "SPZaLrQVAZV4",
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/drive')\n",
        "\n",
        "drive_path = '/content/drive/My Drive/CarData/'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l8QX28rPHzZH",
        "outputId": "dbd2dcc5-82f7-4dac-882e-3399451969bc"
      },
      "id": "l8QX28rPHzZH",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "ca547f91",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-11-04T21:07:06.308385Z",
          "start_time": "2023-11-04T21:07:06.293404Z"
        },
        "id": "ca547f91"
      },
      "outputs": [],
      "source": [
        "parts = {10: {'col': 'orange', 'name':'hood'},\n",
        "         20: {'col':'darkgreen', 'name':'front door'},\n",
        "         30: {'col':'yellow', 'name':'rear door'},\n",
        "         40: {'col':'cyan', 'name':'frame'},\n",
        "         50: {'col':'purple', 'name':'rear quarter panel'},\n",
        "         60: {'col':'lightgreen', 'name':'trunk lid'},\n",
        "         70: {'col':'blue', 'name':'fender'},\n",
        "         80: {'col':'pink', 'name':'bumper'},\n",
        "         90: {'col':'darkgray', 'name':'rest of car'},\n",
        "         0 : {'col':'black', 'name':'background'}}\n",
        "\n",
        "def display_car(data_arr):\n",
        "    # Can take both full data and already split data\n",
        "    if type(data_arr) == torch.Tensor: data_arr = np.moveaxis(data_arr.numpy().astype(np.uint8), 0, 2)\n",
        "    elif data_arr.shape[0] == 3: data_arr = np.moveaxis(data_arr.astype(np.uint8), 0, 2)\n",
        "    elif data_arr.shape[2] > 3: data_arr = data_arr[:,:,:3]\n",
        "    img = Image.fromarray(data_arr)\n",
        "    display(img) # img.show() for jupyter\n",
        "\n",
        "def display_labels(data_arr):\n",
        "    # Can take both full data and already split data\n",
        "    if type(data_arr) == torch.Tensor: data_arr = data_arr.numpy()\n",
        "    if data_arr.dtype != np.uint8: data_arr = data_arr.astype(np.uint8)*10\n",
        "    if data_arr.ndim > 2: data_arr = data_arr[:,:,3]\n",
        "    img = Image.fromarray(data_arr)\n",
        "    pixels = list(img.getdata())\n",
        "    pixels = [ImageColor.getrgb(parts.get(pixel)['col']) for pixel in pixels]\n",
        "    image = Image.new(\"RGB\", (256, 256), (0,0,0))\n",
        "    image.putdata(pixels)\n",
        "    display(image)\n",
        "\n",
        "def numpy_to_tensor(arr):\n",
        "    return np.moveaxis(arr, 2, 0).astype(np.float32)\n",
        "\n",
        "def tensor_to_numpy(tens):\n",
        "    arr = np.moveaxis(tens, 0, 2).astype(np.uint8)\n",
        "    return arr"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def add_background(car, labels, background):\n",
        "    mask = (labels == 0)\n",
        "    updated_car = np.where(mask[..., None], background, car)\n",
        "    updated_car = updated_car.squeeze()\n",
        "\n",
        "    return updated_car\n"
      ],
      "metadata": {
        "id": "uIt50DdGN7ZW"
      },
      "id": "uIt50DdGN7ZW",
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def center_square(img):\n",
        "    \"\"\"Returns the cropped central square of an image (crops the largest dimension to match the smallest one)\"\"\"\n",
        "    if img.size[0] == img.size[1]: return img\n",
        "    smallest_dim = np.argmin(img.size)\n",
        "    largest_dim = np.argmax(img.size)\n",
        "    square_dim = img.size[smallest_dim]\n",
        "    crop_dims = [0,0,0,0]\n",
        "    crop_dims[largest_dim] = int(img.size[largest_dim]/2-square_dim/2)\n",
        "    crop_dims[largest_dim+2] = int(img.size[largest_dim]/2+square_dim/2)\n",
        "    crop_dims[smallest_dim] = 0\n",
        "    crop_dims[smallest_dim+2] = img.size[smallest_dim]\n",
        "    crop_img = img.crop(crop_dims)\n",
        "\n",
        "    return crop_img\n",
        "\n",
        "def set_background(car_arr, labels_arr, img):\n",
        "    \"\"\"Places all non-0 pixels of the car on the background img\"\"\"\n",
        "    center_img = center_square(img)\n",
        "    back_arr = np.array(center_img.resize(labels_arr.shape))\n",
        "    # Use both car and labels just in case\n",
        "    back_arr[labels_arr!=0] = car_arr[labels_arr!=0]\n",
        "\n",
        "    # In the black car dataset, label pixel count should be similar to non-black pixel count\n",
        "    if np.sum(car_arr!=0)/3 < np.sum(labels_arr!=0)*1.2:\n",
        "        # In the black dataset, part of the car isn't correctly labeled, so also use car data for setting background\n",
        "        back_arr[car_arr!=0] = car_arr[car_arr!=0]\n",
        "\n",
        "    return back_arr\n",
        "\n",
        "def move_full_car(arr, x, y, angle=0):\n",
        "    \"\"\"Moves the center of the car to (x, y). Takes the whole array (car AND labels)\"\"\"\n",
        "    car_idxs = np.where(arr!=0)\n",
        "    car_bbox = [max(0,np.min(car_idxs[1])-10), max(0,np.min(car_idxs[0])-10), min(255, np.max(car_idxs[1])+10), min(255,np.max(car_idxs[0])+10)]\n",
        "    # Array with just the car\n",
        "    car_arr = arr[car_bbox[1]:car_bbox[3],car_bbox[0]:car_bbox[2]]\n",
        "    # Rotate the car\n",
        "    car_arr = ndimage.rotate(car_arr, angle, reshape=True, order=0)\n",
        "    # Edges of the car in the new array (without taking into account new image borders)\n",
        "    edges = [y-np.ceil(car_arr.shape[0]/2),y+np.floor(car_arr.shape[0]/2),x-np.ceil(car_arr.shape[1]/2),x+np.floor(car_arr.shape[1]/2)]\n",
        "    # Where to crop the car if it goes off bounds\n",
        "    car_limits = [max(0,-1*int(edges[0])), 255-int(edges[1]) if 255-int(edges[1]) < 0 else car_arr.shape[0], max(0,-1*int(edges[2])), 255-int(edges[3]) if 255-int(edges[3]) < 0 else car_arr.shape[1]]\n",
        "    edges = [max(0,int(edges[0])), min(255, int(edges[1])), max(0,int(edges[2])), min(255, int(edges[3]))]\n",
        "\n",
        "    new_arr = np.zeros(arr.shape)\n",
        "    new_arr[edges[0]:edges[1],edges[2]:edges[3]] = car_arr[car_limits[0]:car_limits[1],car_limits[2]:car_limits[3]]\n",
        "\n",
        "    return new_arr.astype(np.uint8)"
      ],
      "metadata": {
        "id": "IvkXsjno8qvO"
      },
      "id": "IvkXsjno8qvO",
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "02d48b75",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-11-04T21:07:10.043135Z",
          "start_time": "2023-11-04T21:07:10.033140Z"
        },
        "id": "02d48b75"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset\n",
        "from skimage.transform import resize\n",
        "from PIL import Image\n",
        "import random\n",
        "\n",
        "class CarDataset(Dataset):\n",
        "    def __init__(self, root, file_list: list=None, backgrounds: list=None, background_root: str=None, move_car: bool=False, rotate_car: bool=False):\n",
        "\n",
        "        self.root = root\n",
        "        self.filenames = os.listdir(self.root) if file_list is None else file_list\n",
        "        if backgrounds is not None:\n",
        "          self.backgrounds = backgrounds\n",
        "        else:\n",
        "          self.backgrounds = None\n",
        "        if background_root is not None:\n",
        "            self.backgrounds = [os.path.join(background_root, filename) for filename in os.listdir(background_root)]\n",
        "        self.move_car = move_car\n",
        "        self.rotate_car = rotate_car\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.filenames)\n",
        "\n",
        "    # def __getitem__(self, index):\n",
        "        # filename = self.filenames[index]\n",
        "        # arr = np.load(os.path.join(self.root, filename))\n",
        "        # car = arr[:,:,0:3]\n",
        "        # labels = arr[:,:,3]\n",
        "\n",
        "        # if self.backgrounds is not None:\n",
        "        #   background = random.choices(self.backgrounds)\n",
        "        #   car = add_background(car, labels, background)\n",
        "\n",
        "        # car = car.astype(np.float32)\n",
        "        # car = np.moveaxis(car, 2, 0)\n",
        "\n",
        "        # return car, labels/10\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        filename = self.filenames[index]\n",
        "        arr = np.load(os.path.join(self.root, filename))\n",
        "        # Don't move car with 10% probability\n",
        "        if self.move_car and random.randrange(0,10)>0:\n",
        "            x = random.randrange(80,160)\n",
        "            y = random.randrange(100,210)\n",
        "            angle = random.randrange(-30,30) if self.rotate_car else 0\n",
        "            arr = move_full_car(arr, x, y, angle)\n",
        "        car = arr[:,:,0:3]\n",
        "        labels = arr[:,:,3]\n",
        "        if self.backgrounds is not None:\n",
        "          background = random.choices(self.backgrounds)\n",
        "          car = add_background(car, labels, background)\n",
        "        # if self.background_root is not None and 'photo' not in filename and random.randrange(0,10)>0:\n",
        "        #     rand_idx = random.randrange(0,len(self.backgrounds))\n",
        "        #     img = Image.open(self.backgrounds[rand_idx]).convert('RGB')\n",
        "        #     car = set_background(car, labels, img)\n",
        "        # Resize function does not work on int dtypes, so we must convert to float before\n",
        "        car = resize(car.astype(np.float32), (256, 256))\n",
        "        # Convert to the torch tensor image convention (CxWxH)\n",
        "        car = np.moveaxis(car, 2, 0)\n",
        "\n",
        "        labels = resize(labels.astype(np.float32), (256, 256))\n",
        "\n",
        "        return torch.tensor(car), torch.tensor(labels/10)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "\n",
        "def load_images_from_folder(folder_path, resize_shape=(256, 256), limit=100):\n",
        "    image_list = []\n",
        "    count = 0\n",
        "\n",
        "    for filename in os.listdir(folder_path):\n",
        "        # Check if the file is an image file (you can add more extensions if needed)\n",
        "        if filename.endswith(\".jpg\") or filename.endswith(\".png\") or filename.endswith(\".jpeg\"):\n",
        "            file_path = os.path.join(folder_path, filename)\n",
        "\n",
        "            # Open the image using PIL\n",
        "            img = Image.open(file_path)\n",
        "\n",
        "            # Resize the image\n",
        "            img = img.resize(resize_shape)\n",
        "\n",
        "            # Convert the image to a numpy array and append to the list\n",
        "            image_array = np.array(img)\n",
        "            image_list.append(image_array)\n",
        "\n",
        "            count += 1\n",
        "            if count >= limit:\n",
        "                break\n",
        "\n",
        "    return image_list\n",
        "\n",
        "# Example usage:\n",
        "folder_path = f'{drive_path}images/landscapes'\n",
        "backgrounds = load_images_from_folder(folder_path, limit=250)\n"
      ],
      "metadata": {
        "id": "GclJrpvISYQD"
      },
      "id": "GclJrpvISYQD",
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "black_car = []\n",
        "orange_car = []\n",
        "photos = []\n",
        "for file in os.listdir(f'{drive_path}arrays'):\n",
        "    if 'orange' in file: orange_car.append(file)\n",
        "    elif 'black' in file: black_car.append(file)\n",
        "    elif 'photo' in file: photos.append(file)\n",
        "\n",
        "print(len(black_car), len(orange_car), len(photos))"
      ],
      "metadata": {
        "id": "CrsuTuMeU969",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aa74d705-e0ad-41ca-9ff9-fc369a71d47d"
      },
      "id": "CrsuTuMeU969",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "834 2001 168\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader, Subset\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "root = f'{drive_path}arrays'\n",
        "\n",
        "# ds = CarDataset(data_root, move_car=True, rotate_car=True, background_root=backgrounds_root)\n",
        "\n",
        "test_set = photos\n",
        "\n",
        "photo_train_val = photos\n",
        "photo_train, photo_val = train_test_split(photo_train_val, test_size=0.2, random_state=42)\n",
        "black_train, black_val = train_test_split(black_car, test_size=0.2, random_state=42)\n",
        "orange_train, orange_val = train_test_split(orange_car, test_size=0.2, random_state=42)\n",
        "\n",
        "# photo_train, photo_test = train_test_split(photos, test_size=0.4, random_state=42)\n",
        "# black_train, black_test = train_test_split(black_car, test_size=0.4, random_state=42)\n",
        "# orange_train, orange_test = train_test_split(orange_car, test_size=0.4, random_state=42)\n",
        "\n",
        "# photo_val, photo_test = train_test_split(photo_test, test_size=0.5, random_state=42)\n",
        "# black_val, black_test = train_test_split(black_test, test_size=0.5, random_state=42)\n",
        "# orange_val, orange_test = train_test_split(orange_test, test_size=0.5, random_state=42)\n",
        "\n",
        "\n",
        "# joint_train_ds = CarDataset(root, photo_train*3+black_train+orange_train)\n",
        "# joint_train_background_ds = CarDataset(root, black_train+orange_train, backgrounds)\n",
        "# joint_val_ds = CarDataset(root, photo_val+black_val+orange_val)\n",
        "\n",
        "# joint_test_ds = CarDataset(root, photo_test+black_test+orange_test)\n",
        "# photo_test_ds = CarDataset(root, photo_test)\n",
        "# black_test_ds = CarDataset(root, black_test)\n",
        "# orange_test_ds = CarDataset(root, orange_test)\n",
        "\n",
        "\n",
        "joint_train_ds = CarDataset(root, photo_train*3+black_train+orange_train)\n",
        "joint_train_background_ds = CarDataset(root, photo_train+black_train+orange_train, backgrounds)\n",
        "joint_val_ds = CarDataset(root, photo_val+black_val+orange_val)\n",
        "\n",
        "# joint_test_ds = CarDataset(root, photo_test+black_test+orange_test)\n",
        "# photo_test_ds = CarDataset(root, photo_test)\n",
        "# black_test_ds = CarDataset(root, black_test)\n",
        "# orange_test_ds = CarDataset(root, orange_test)\n",
        "\n",
        "joint_test_ds = CarDataset(root, test_set)\n",
        "\n",
        "train_loader = DataLoader(joint_train_ds+joint_train_background_ds, batch_size=16, shuffle=True)\n",
        "val_loader = DataLoader(joint_val_ds, batch_size=16)\n",
        "test_loader = DataLoader(joint_test_ds, batch_size=16)\n",
        "\n",
        "# photo_test_loader =  DataLoader(photo_test_ds, batch_size=16)\n",
        "# black_test_loader =  DataLoader(black_test_ds, batch_size=16)\n",
        "# orange_test_loader =  DataLoader(orange_test_ds, batch_size=16)"
      ],
      "metadata": {
        "id": "zRX1q0M_RY9R"
      },
      "id": "zRX1q0M_RY9R",
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "#!pip install torchmetrics\n",
        "from torchmetrics.classification import Dice"
      ],
      "metadata": {
        "id": "uU4bWOLvvyWV"
      },
      "id": "uU4bWOLvvyWV",
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ConvBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, batch_norm=True, encoder=False, leaky_relu_slope=0.2):\n",
        "        super().__init__()\n",
        "\n",
        "        layers = [\n",
        "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1, stride=2),\n",
        "            nn.BatchNorm2d(out_channels) if batch_norm else nn.Identity(),\n",
        "            nn.LeakyReLU(leaky_relu_slope) if encoder else nn.ReLU(),\n",
        "            nn.Dropout2d(0.3),\n",
        "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1, stride=2),\n",
        "            nn.BatchNorm2d(out_channels) if batch_norm else nn.Identity(),\n",
        "            nn.LeakyReLU(leaky_relu_slope) if encoder else nn.ReLU()\n",
        "        ]\n",
        "\n",
        "        self.block = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.block(x)\n",
        "\n",
        "class UNet(nn.Module):\n",
        "    def __init__(self, in_channels, num_classes):\n",
        "        super(UNet, self).__init__()\n",
        "\n",
        "        self.encoder0 = ConvBlock(in_channels, 64, batch_norm=False, encoder=True)\n",
        "        self.encoder1 = ConvBlock(64, 128, encoder=True)\n",
        "        self.encoder2 = ConvBlock(128, 256, encoder=True)\n",
        "        self.encoder3 = ConvBlock(256, 512, encoder=True)\n",
        "        self.encoder4 = ConvBlock(512, 512, encoder=True)\n",
        "        self.encoder5 = ConvBlock(512, 512, encoder=True)\n",
        "        self.encoder6 = ConvBlock(512, 512, encoder=True)\n",
        "        self.encoder7 = ConvBlock(512, 512, encoder=True)\n",
        "\n",
        "        self.bottleneck = ConvBlock(512, 512)\n",
        "        self.bottleneck_upsample = nn.ConvTranspose2d(512, 512, kernel_size=1, stride=1)\n",
        "\n",
        "        self.decoder7 = ConvBlock(1024, 512)\n",
        "        self.decoder6 = ConvBlock(1024, 512)\n",
        "        self.decoder5 = ConvBlock(1024, 512)\n",
        "        self.decoder4 = ConvBlock(1024, 512)\n",
        "        self.decoder3 = ConvBlock(1024, 256)\n",
        "        self.decoder2 = ConvBlock(512, 128)\n",
        "        self.decoder1 = ConvBlock(256, 64)\n",
        "        self.decoder0 = ConvBlock(64, num_classes)\n",
        "\n",
        "        self.final_conv = nn.Conv2d(num_classes, num_classes, kernel_size=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x0 = self.encoder0(x)\n",
        "        x1 = self.encoder1(x0)\n",
        "        x2 = self.encoder2(x1)\n",
        "        x3 = self.encoder3(x2)\n",
        "        x4 = self.encoder4(x3)\n",
        "        x5 = self.encoder5(x4)\n",
        "        x6 = self.encoder6(x5)\n",
        "        x7 = self.encoder7(x6)\n",
        "\n",
        "        bottleneck = self.bottleneck(x7)\n",
        "        bottleneck_upsampled = self.bottleneck_upsample(bottleneck)\n",
        "\n",
        "        x7_d = self.decoder7(torch.cat([x7, bottleneck_upsampled], dim=1))\n",
        "        x6_d = self.decoder6(torch.cat([x6, x7_d], dim=1))\n",
        "        x5_d = self.decoder5(torch.cat([x5, x6_d], dim=1))\n",
        "        x4_d = self.decoder4(torch.cat([x4, x5_d], dim=1))\n",
        "        x3_d = self.decoder3(torch.cat([x3, x4_d], dim=1))\n",
        "        x3_d_upsampled = nn.Upsample(scale_factor=4, mode='nearest')(x3_d)\n",
        "        x2_d = self.decoder2(torch.cat([x2, x3_d_upsampled], dim=1))\n",
        "        x2_d_upsampled = nn.Upsample(scale_factor=16, mode='nearest')(x2_d)\n",
        "        x1_d = self.decoder1(torch.cat([x1, x2_d_upsampled], dim=1))\n",
        "        x1_d_upsampled = nn.Upsample(scale_factor=16, mode='nearest')(x1_d)\n",
        "        x0_d = self.decoder0(x1_d_upsampled)\n",
        "        x0_d_upsampled = nn.Upsample(scale_factor=16, mode='nearest')(x0_d)\n",
        "\n",
        "        output = self.final_conv(x0_d_upsampled)\n",
        "\n",
        "        return output\n",
        "\n"
      ],
      "metadata": {
        "id": "1kNd_9agvsB5"
      },
      "id": "1kNd_9agvsB5",
      "execution_count": 138,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import torch.nn as nn\n",
        "\n",
        "# class ConvBlock(nn.Module):\n",
        "#     def __init__(self, in_channels, out_channels):\n",
        "#         super().__init__()\n",
        "#         self.block = nn.Sequential(\n",
        "#             nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
        "#             nn.BatchNorm2d(out_channels),\n",
        "#             nn.ReLU(),\n",
        "#             nn.Dropout2d(0.3),\n",
        "#             nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
        "#             nn.BatchNorm2d(out_channels),\n",
        "#             nn.ReLU()\n",
        "#         )\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         return self.block(x)\n",
        "\n",
        "# class UNet(nn.Module):\n",
        "#     def __init__(self, in_channels, num_classes):\n",
        "#         super(UNet, self).__init__()\n",
        "#         self.encoder0 = nn.Sequential(ConvBlock(in_channels, 64))\n",
        "#         self.encoder1 = nn.Sequential(nn.MaxPool2d(2,2), ConvBlock(64, 128))\n",
        "#         self.encoder2 = nn.Sequential(nn.MaxPool2d(2,2), ConvBlock(128, 256))\n",
        "#         self.bottleneck = nn.Sequential(nn.MaxPool2d(2,2), ConvBlock(256,512), nn.ConvTranspose2d(512, 256, kernel_size=2, stride=2))\n",
        "#         self.decoder0 = nn.Sequential(ConvBlock(512,256), nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2))\n",
        "#         self.decoder1 = nn.Sequential(ConvBlock(256,128), nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2))\n",
        "#         self.decoder2 = nn.Sequential(ConvBlock(128,64), nn.Conv2d(in_channels=64, out_channels=num_classes, kernel_size=1))\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         x0 = self.encoder0(x)\n",
        "#         x1 = self.encoder1(x0)\n",
        "#         x2 = self.encoder2(x1)\n",
        "#         x3 = self.bottleneck(x2)\n",
        "#         x3 = self.decoder0(torch.cat([x2,x3],dim=1))\n",
        "#         x3 = self.decoder1(torch.cat([x1,x3],dim=1))\n",
        "#         x3 = self.decoder2(torch.cat([x0,x3],dim=1))\n",
        "#         # print(\"x3.shape: \", x3.shape)\n",
        "\n",
        "#         return x3\n"
      ],
      "metadata": {
        "id": "U4HG523Eds7l"
      },
      "id": "U4HG523Eds7l",
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "plY9UhFDTMI5"
      },
      "id": "plY9UhFDTMI5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vq0pXlQem84B"
      },
      "id": "vq0pXlQem84B",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class PatchGANDiscriminator(nn.Module):\n",
        "    def __init__(self, in_channels=1):\n",
        "        super(PatchGANDiscriminator, self).__init__()\n",
        "\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, 16, kernel_size=4, stride=2, padding=1),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Conv2d(16, 128, kernel_size=4, stride=2, padding=1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Conv2d(256, 512, kernel_size=4, stride=2, padding=1),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Conv2d(512, 1, kernel_size=4, stride=1, padding=0),  # Adjust kernel_size and stride\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.model(x)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "KfLxEjbxZh8p"
      },
      "id": "KfLxEjbxZh8p",
      "execution_count": 139,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JyP4SN7qb1bU"
      },
      "id": "JyP4SN7qb1bU",
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\"\n",
        "generator = UNet(3, 10).to(device)\n",
        "discriminator = PatchGANDiscriminator().to(device)\n"
      ],
      "metadata": {
        "id": "mTOh4IglxbHw"
      },
      "id": "mTOh4IglxbHw",
      "execution_count": 140,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "criterion_L1 = nn.L1Loss()\n",
        "criterion_adv = nn.BCEWithLogitsLoss()\n",
        "\n",
        "optimizer_gen = optim.Adam(generator.parameters(), lr=0.001, weight_decay=1e-5)\n",
        "optimizer_dis = optim.Adam(discriminator.parameters(), lr=0.001, weight_decay=1e-5)"
      ],
      "metadata": {
        "id": "PeWijAip7nDh"
      },
      "id": "PeWijAip7nDh",
      "execution_count": 141,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8ugfQCb45aC-"
      },
      "id": "8ugfQCb45aC-",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 25\n",
        "dice = Dice(average='micro').to(device)\n",
        "loaders = [train_loader, ]\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    generator.train()\n",
        "    discriminator.train()\n",
        "\n",
        "    total_gen_loss = 0.0\n",
        "    total_dis_loss = 0.0\n",
        "\n",
        "    for batch in train_loader:\n",
        "        inputs, labels = batch\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        inputs = inputs.float()\n",
        "        labels = labels.long()\n",
        "\n",
        "        # Train the discriminator\n",
        "        optimizer_dis.zero_grad()\n",
        "        fake_targets = generator(inputs)\n",
        "        loss_gen_L1 = criterion(fake_targets, labels)\n",
        "\n",
        "        fake_targets = torch.argmax(fake_targets, dim=1, keepdim=True)\n",
        "\n",
        "        labels = labels.unsqueeze(1)\n",
        "        labels = labels.float()\n",
        "\n",
        "        real_preds = discriminator(labels)\n",
        "        fake_preds = discriminator(fake_targets.detach().float())\n",
        "        real_loss = criterion_adv(real_preds, torch.ones_like(real_preds))\n",
        "        fake_loss = criterion_adv(fake_preds, torch.zeros_like(fake_preds))\n",
        "        loss_dis = 0.5 * (real_loss + fake_loss)\n",
        "        loss_dis.backward()\n",
        "        optimizer_dis.step()\n",
        "\n",
        "        # Train the generator\n",
        "        optimizer_gen.zero_grad()\n",
        "        fake_t_pred = discriminator(fake_targets.float())\n",
        "        loss_gen_adv = criterion_adv(fake_t_pred, torch.ones_like(fake_t_pred))\n",
        "\n",
        "        # loss_gen = 0.4*loss_gen_adv + 0.8 * loss_gen_L1\n",
        "        loss_gen = 0.5*loss_gen_adv + 0.9 * loss_gen_L1\n",
        "        loss_gen.backward()\n",
        "        optimizer_gen.step()\n",
        "\n",
        "        total_dis_loss += loss_dis.item()\n",
        "        total_gen_loss += loss_gen.item()\n",
        "\n",
        "    avg_dis_loss = total_dis_loss / len(train_loader)\n",
        "    avg_gen_loss = total_gen_loss / len(train_loader)\n",
        "    print(f\"Epoch {epoch + 1}/{num_epochs}, Generator Loss: {avg_gen_loss}, Discriminator Loss: {avg_dis_loss}\")\n",
        "\n",
        "    generator.eval()\n",
        "    total_val_loss = 0.0\n",
        "    total_dice = 0.0\n",
        "    for batch in val_loader:\n",
        "        inputs, labels = batch\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        inputs = inputs.float()\n",
        "        labels = labels.long()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            val_outputs = generator(inputs)\n",
        "            val_loss = criterion(val_outputs, labels)\n",
        "            val_outputs = val_outputs.to(device)\n",
        "            total_val_loss += val_loss.item()\n",
        "            val_dice = dice(val_outputs, labels)\n",
        "            total_dice += val_dice\n",
        "\n",
        "    average_val_loss = total_val_loss / len(val_loader)\n",
        "    average_dice = total_dice / len(val_loader)\n",
        "    print(f\"Epoch {epoch + 1}/{num_epochs}, Generator Loss (Validation): {average_val_loss:.4f}, Dice: {average_dice:.4f}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "pieMNBpcD2VY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5a4cc4e0-94eb-4df2-c5cc-5261eb5638e1"
      },
      "id": "pieMNBpcD2VY",
      "execution_count": 142,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/25, Generator Loss: 1.4692795910293748, Discriminator Loss: 0.5536835171070761\n",
            "Epoch 1/25, Generator Loss (Validation): 0.6284, Dice: 0.8286\n",
            "Epoch 2/25, Generator Loss: 0.9041849989620293, Discriminator Loss: 0.5124955953859757\n",
            "Epoch 2/25, Generator Loss (Validation): 0.5076, Dice: 0.8419\n",
            "Epoch 3/25, Generator Loss: 0.8544845376104963, Discriminator Loss: 0.5043127059184415\n",
            "Epoch 3/25, Generator Loss (Validation): 0.4814, Dice: 0.8446\n",
            "Epoch 4/25, Generator Loss: 0.8295241966608571, Discriminator Loss: 0.5062632923622613\n",
            "Epoch 4/25, Generator Loss (Validation): 0.4577, Dice: 0.8469\n",
            "Epoch 5/25, Generator Loss: 0.8144570154722557, Discriminator Loss: 0.503340592519718\n",
            "Epoch 5/25, Generator Loss (Validation): 0.4507, Dice: 0.8475\n",
            "Epoch 6/25, Generator Loss: 0.7994700100519679, Discriminator Loss: 0.5032298653283706\n",
            "Epoch 6/25, Generator Loss (Validation): 0.4346, Dice: 0.8513\n",
            "Epoch 7/25, Generator Loss: 0.7894137544962886, Discriminator Loss: 0.5032190932460388\n",
            "Epoch 7/25, Generator Loss (Validation): 0.4235, Dice: 0.8539\n",
            "Epoch 8/25, Generator Loss: 0.7786766271861946, Discriminator Loss: 0.5032128155795558\n",
            "Epoch 8/25, Generator Loss (Validation): 0.4147, Dice: 0.8582\n",
            "Epoch 9/25, Generator Loss: 0.7756025358705491, Discriminator Loss: 0.5032095111883025\n",
            "Epoch 9/25, Generator Loss (Validation): 0.4097, Dice: 0.8582\n",
            "Epoch 10/25, Generator Loss: 0.7660927889098881, Discriminator Loss: 0.5069289651202855\n",
            "Epoch 10/25, Generator Loss (Validation): 0.4112, Dice: 0.8578\n",
            "Epoch 11/25, Generator Loss: 0.766851709092077, Discriminator Loss: 0.5032390085304573\n",
            "Epoch 11/25, Generator Loss (Validation): 0.3990, Dice: 0.8586\n",
            "Epoch 12/25, Generator Loss: 0.7565095853354277, Discriminator Loss: 0.5032173051066955\n",
            "Epoch 12/25, Generator Loss (Validation): 0.3905, Dice: 0.8614\n",
            "Epoch 13/25, Generator Loss: 0.7516344409635767, Discriminator Loss: 0.5032132619562961\n",
            "Epoch 13/25, Generator Loss (Validation): 0.3876, Dice: 0.8615\n",
            "Epoch 14/25, Generator Loss: 0.7460713316965555, Discriminator Loss: 0.503214791370115\n",
            "Epoch 14/25, Generator Loss (Validation): 0.3830, Dice: 0.8630\n",
            "Epoch 15/25, Generator Loss: 0.7491878987486806, Discriminator Loss: 0.5032167329397111\n",
            "Epoch 15/25, Generator Loss (Validation): 0.3833, Dice: 0.8607\n",
            "Epoch 16/25, Generator Loss: 0.7385866841307198, Discriminator Loss: 0.5171079381783302\n",
            "Epoch 16/25, Generator Loss (Validation): 0.3810, Dice: 0.8618\n",
            "Epoch 17/25, Generator Loss: 0.7397310192276627, Discriminator Loss: 0.50342818502372\n",
            "Epoch 17/25, Generator Loss (Validation): 0.3751, Dice: 0.8632\n",
            "Epoch 18/25, Generator Loss: 0.7354801900755343, Discriminator Loss: 0.5032147007409706\n",
            "Epoch 18/25, Generator Loss (Validation): 0.3720, Dice: 0.8645\n",
            "Epoch 19/25, Generator Loss: 0.7370448537429425, Discriminator Loss: 0.5032097215908183\n",
            "Epoch 19/25, Generator Loss (Validation): 0.3825, Dice: 0.8610\n",
            "Epoch 20/25, Generator Loss: 0.7382563785047561, Discriminator Loss: 0.5040722750714901\n",
            "Epoch 20/25, Generator Loss (Validation): 0.3763, Dice: 0.8612\n",
            "Epoch 21/25, Generator Loss: 0.7326553902039392, Discriminator Loss: 0.5032175138169659\n",
            "Epoch 21/25, Generator Loss (Validation): 0.3708, Dice: 0.8641\n",
            "Epoch 22/25, Generator Loss: 0.7294509854602513, Discriminator Loss: 0.5051721011050492\n",
            "Epoch 22/25, Generator Loss (Validation): 0.3753, Dice: 0.8633\n",
            "Epoch 23/25, Generator Loss: 0.7284435471901758, Discriminator Loss: 0.5032132487943872\n",
            "Epoch 23/25, Generator Loss (Validation): 0.3705, Dice: 0.8656\n",
            "Epoch 24/25, Generator Loss: 0.7255869838717608, Discriminator Loss: 0.5032094152943946\n",
            "Epoch 24/25, Generator Loss (Validation): 0.3654, Dice: 0.8646\n",
            "Epoch 25/25, Generator Loss: 0.728708234495169, Discriminator Loss: 0.503209545597293\n",
            "Epoch 25/25, Generator Loss (Validation): 0.3673, Dice: 0.8642\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1hTdpUFwiFby"
      },
      "id": "1hTdpUFwiFby",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8R14hFtZD2XW"
      },
      "id": "8R14hFtZD2XW",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xQaY9sY_cI_V"
      },
      "id": "xQaY9sY_cI_V",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XJ_nju9sx2RG"
      },
      "id": "XJ_nju9sx2RG",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# torch.save(model.state_dict(), f'{drive_path}model2.pth')"
      ],
      "metadata": {
        "id": "nSig9FTq9wpq"
      },
      "id": "nSig9FTq9wpq",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Original\n",
        "\n",
        "Photos: 0.8259544\\\n",
        "Black: 0.9816798\\\n",
        "Orange: 0.98628855\\\n",
        "Test: 0.9728253"
      ],
      "metadata": {
        "id": "jO5exdysxHFk"
      },
      "id": "jO5exdysxHFk"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Added photos\n",
        "\n",
        "Photos: 0.83027714\\\n",
        "Black: 0.9769864\\\n",
        "Orange: 0.98165745\\\n",
        "Test: 0.9720461"
      ],
      "metadata": {
        "id": "04qmVMsGGH9h"
      },
      "id": "04qmVMsGGH9h"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Added photos and background\n",
        "\n",
        "Photos: 0.8418195\\\n",
        "Black: 0.9842939\\\n",
        "Orange: 0.9854847\\\n",
        "Test: 0.97684497\n",
        "\n"
      ],
      "metadata": {
        "id": "kGmyT6xBIg_9"
      },
      "id": "kGmyT6xBIg_9"
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RKgLcOX-iNXY"
      },
      "id": "RKgLcOX-iNXY",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test weighted Cross Entropy Loss/Cross Entropy Loss/Dice Loss/Jaccard Loss"
      ],
      "metadata": {
        "id": "B-wv-wpiD3fu"
      },
      "id": "B-wv-wpiD3fu",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test different values for learning rate, weight decay etc."
      ],
      "metadata": {
        "id": "qNU4qTUpD3hv"
      },
      "id": "qNU4qTUpD3hv",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mIcLx2PnEpvQ"
      },
      "id": "mIcLx2PnEpvQ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VTGMLIj0EGJM"
      },
      "id": "VTGMLIj0EGJM",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.10"
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}