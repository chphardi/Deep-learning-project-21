{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "207e05ac",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-11-04T20:36:30.529567Z",
          "start_time": "2023-11-04T20:36:30.510549Z"
        },
        "id": "207e05ac",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2688ab29-e943-463a-a031-abbebc4a5736"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torchmetrics\n",
            "  Downloading torchmetrics-1.2.0-py3-none-any.whl (805 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m805.2/805.2 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>1.20.0 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (1.23.5)\n",
            "Requirement already satisfied: torch>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (2.1.0+cu118)\n",
            "Collecting lightning-utilities>=0.8.0 (from torchmetrics)\n",
            "  Downloading lightning_utilities-0.10.0-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: packaging>=17.1 in /usr/local/lib/python3.10/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (23.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (67.7.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (4.5.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->torchmetrics) (3.13.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->torchmetrics) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->torchmetrics) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->torchmetrics) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->torchmetrics) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->torchmetrics) (2.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.8.1->torchmetrics) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.8.1->torchmetrics) (1.3.0)\n",
            "Installing collected packages: lightning-utilities, torchmetrics\n",
            "Successfully installed lightning-utilities-0.10.0 torchmetrics-1.2.0\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from google.colab import drive\n",
        "import skimage.io as io\n",
        "import numpy as np\n",
        "import os\n",
        "import torch\n",
        "import torchvision.transforms as T\n",
        "from PIL import Image, ImageColor\n",
        "from scipy import ndimage\n",
        "from torch.utils.data import Dataset\n",
        "from skimage.transform import resize\n",
        "import random\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "!pip install torchmetrics\n",
        "from torchmetrics.classification import Dice\n",
        "import torch.nn.init as init"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/drive')\n",
        "drive_path = 'drive/My Drive/'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l8QX28rPHzZH",
        "outputId": "4b75a184-d79b-466d-b428-f083f131672a"
      },
      "id": "l8QX28rPHzZH",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "ca547f91",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-11-04T21:07:06.308385Z",
          "start_time": "2023-11-04T21:07:06.293404Z"
        },
        "id": "ca547f91"
      },
      "outputs": [],
      "source": [
        "parts = {10: {'col': 'orange', 'name':'hood'},\n",
        "         20: {'col':'darkgreen', 'name':'front door'},\n",
        "         30: {'col':'yellow', 'name':'rear door'},\n",
        "         40: {'col':'cyan', 'name':'frame'},\n",
        "         50: {'col':'purple', 'name':'rear quarter panel'},\n",
        "         60: {'col':'lightgreen', 'name':'trunk lid'},\n",
        "         70: {'col':'blue', 'name':'fender'},\n",
        "         80: {'col':'pink', 'name':'bumper'},\n",
        "         90: {'col':'darkgray', 'name':'rest of car'},\n",
        "         0 : {'col':'black', 'name':'background'}}\n",
        "\n",
        "def display_car(data_arr):\n",
        "    # Can take both full data and already split data\n",
        "    if type(data_arr) == torch.Tensor: data_arr = np.moveaxis(data_arr.numpy().astype(np.uint8), 0, 2)\n",
        "    elif data_arr.shape[0] == 3: data_arr = np.moveaxis(data_arr.astype(np.uint8), 0, 2)\n",
        "    elif data_arr.shape[2] > 3: data_arr = data_arr[:,:,:3]\n",
        "    img = Image.fromarray(data_arr)\n",
        "    display(img) # img.show() for jupyter\n",
        "\n",
        "def display_labels(data_arr):\n",
        "    # Can take both full data and already split data\n",
        "    if type(data_arr) == torch.Tensor: data_arr = data_arr.numpy()\n",
        "    if data_arr.dtype != np.uint8: data_arr = data_arr.astype(np.uint8)*10\n",
        "    if data_arr.ndim > 2: data_arr = data_arr[:,:,3]\n",
        "    img = Image.fromarray(data_arr)\n",
        "    pixels = list(img.getdata())\n",
        "    pixels = [ImageColor.getrgb(parts.get(pixel)['col']) for pixel in pixels]\n",
        "    image = Image.new(\"RGB\", (256, 256), (0,0,0))\n",
        "    image.putdata(pixels)\n",
        "    display(image)\n",
        "\n",
        "def numpy_to_tensor(arr):\n",
        "    return np.moveaxis(arr, 2, 0).astype(np.float32)\n",
        "\n",
        "def tensor_to_numpy(tens):\n",
        "    arr = np.moveaxis(tens, 0, 2).astype(np.uint8)\n",
        "    return arr"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def center_square(img):\n",
        "    \"\"\"Returns the cropped central square of an image (crops the largest dimension to match the smallest one)\"\"\"\n",
        "    if img.size[0] == img.size[1]: return img\n",
        "    smallest_dim = np.argmin(img.size)\n",
        "    largest_dim = np.argmax(img.size)\n",
        "    square_dim = img.size[smallest_dim]\n",
        "    crop_dims = [0,0,0,0]\n",
        "    crop_dims[largest_dim] = int(img.size[largest_dim]/2-square_dim/2)\n",
        "    crop_dims[largest_dim+2] = int(img.size[largest_dim]/2+square_dim/2)\n",
        "    crop_dims[smallest_dim] = 0\n",
        "    crop_dims[smallest_dim+2] = img.size[smallest_dim]\n",
        "    crop_img = img.crop(crop_dims)\n",
        "\n",
        "    return crop_img\n",
        "\n",
        "def set_background(car_arr, labels_arr, img):\n",
        "    \"\"\"Places all non-0 pixels of the car on the background img\"\"\"\n",
        "    center_img = center_square(img)\n",
        "    back_arr = np.array(center_img.resize(labels_arr.shape))\n",
        "    # Use both car and labels just in case\n",
        "    back_arr[labels_arr!=0] = car_arr[labels_arr!=0]\n",
        "\n",
        "    # In the black car dataset, label pixel count should be similar to non-black pixel count\n",
        "    if np.sum(car_arr!=0)/3 < np.sum(labels_arr!=0)*1.2:\n",
        "        # In the black dataset, part of the car isn't correctly labeled, so also use car data for setting background\n",
        "        back_arr[car_arr!=0] = car_arr[car_arr!=0]\n",
        "\n",
        "    return back_arr\n",
        "\n",
        "def move_full_car(arr, x, y, angle=0, zoom=1):\n",
        "    \"\"\"Moves the center of the car to (x, y). Takes the whole array (car AND labels)\"\"\"\n",
        "    car_idxs = np.where(arr!=0)\n",
        "    car_bbox = [max(0,np.min(car_idxs[1])-10), max(0,np.min(car_idxs[0])-10), min(255, np.max(car_idxs[1])+10), min(255,np.max(car_idxs[0])+10)]\n",
        "    # Array with just the car\n",
        "    car_arr = arr[car_bbox[1]:car_bbox[3],car_bbox[0]:car_bbox[2]]\n",
        "    # Rotate the car\n",
        "    car_arr = ndimage.rotate(car_arr, angle, reshape=True, order=0)\n",
        "    car_arr = ndimage.zoom(car_arr, (zoom, zoom, 1), order=0)\n",
        "    # Edges of the car in the new array (without taking into account new image borders)\n",
        "    edges = [y-np.ceil(car_arr.shape[0]/2),y+np.floor(car_arr.shape[0]/2),x-np.ceil(car_arr.shape[1]/2),x+np.floor(car_arr.shape[1]/2)]\n",
        "    # Where to crop the car if it goes off bounds\n",
        "    car_limits = [max(0,-1*int(edges[0])), 255-int(edges[1]) if 255-int(edges[1]) < 0 else car_arr.shape[0], max(0,-1*int(edges[2])), 255-int(edges[3]) if 255-int(edges[3]) < 0 else car_arr.shape[1]]\n",
        "    edges = [max(0,int(edges[0])), min(255, int(edges[1])), max(0,int(edges[2])), min(255, int(edges[3]))]\n",
        "\n",
        "    new_arr = np.zeros(arr.shape)\n",
        "    new_arr[edges[0]:edges[1],edges[2]:edges[3]] = car_arr[car_limits[0]:car_limits[1],car_limits[2]:car_limits[3]]\n",
        "\n",
        "    return new_arr.astype(np.uint8)\n"
      ],
      "metadata": {
        "id": "7yikSHC1CRkE"
      },
      "id": "7yikSHC1CRkE",
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_images_from_folder(folder_path, resize_shape=(256, 256), limit=100):\n",
        "    background_list = []\n",
        "    count = 0\n",
        "\n",
        "    for filename in os.listdir(folder_path):\n",
        "        # Check if the file is an image file\n",
        "        file_path = os.path.join(folder_path, filename)\n",
        "\n",
        "        background = Image.open(file_path).convert('RGB')\n",
        "        background_list.append(background)\n",
        "\n",
        "        count += 1\n",
        "        if count >= limit:\n",
        "            break\n",
        "\n",
        "    return background_list\n",
        "\n",
        "# Example usage:\n",
        "folder_path = 'drive/My Drive/carseg_data/images/landscapes'\n",
        "background_list = load_images_from_folder(folder_path, limit=250)"
      ],
      "metadata": {
        "id": "qeEpKGNqKlJ9"
      },
      "id": "qeEpKGNqKlJ9",
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CarDataset(Dataset):\n",
        "    def __init__(self, root, file_list: list=None, backgrounds: list=[], move_car: bool=False, rotate_car: bool=False, zoom_car: bool=False):\n",
        "        \"\"\"\n",
        "        Initializes the dataset.\n",
        "        Parameters:\n",
        "            file_list: a list of filenames from 'root' to use. If not specified, all files will be used.\n",
        "            background: list with backgrounds. If not specified, no backgrounds will be used.\n",
        "            move_car: specifies if the cars should be moved to a random location in the image\n",
        "            rotate_car: specifies if the cars should be given a random rotation (within a range)\n",
        "        Backgrounds, rotations and translations are random. There is a chance that none will be performed at all.\n",
        "        This chance is higher for 'photo' images, which will only be rotated/translated when the background is changed (to avoid black bars)\n",
        "        \"\"\"\n",
        "        self.root = root\n",
        "        self.filenames = os.listdir(self.root) if file_list is None else file_list\n",
        "        self.backgrounds = backgrounds\n",
        "        self.move_car = move_car\n",
        "        self.rotate_car = rotate_car\n",
        "        self.zoom_car = zoom_car\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.filenames)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        filename = self.filenames[index]\n",
        "        arr = np.load(os.path.join(self.root, filename))\n",
        "        photo_mod = True\n",
        "        if 'photo' in filename:\n",
        "            # Photos only get new background with 33% chance\n",
        "            photo_mod = random.randrange(0,3)==1\n",
        "\n",
        "        if self.move_car:\n",
        "            x = random.randrange(80,255-80)\n",
        "            y = random.randrange(80,255-80)\n",
        "            angle = random.randrange(-30,30) if self.rotate_car else 0\n",
        "            zoom = random.uniform(0.8,1.4) if self.zoom_car else 1\n",
        "            arr = move_full_car(arr, x, y, angle, zoom)\n",
        "\n",
        "        car = arr[:,:,0:3]\n",
        "        labels = arr[:,:,3]\n",
        "\n",
        "        if len(self.backgrounds) > 0 and photo_mod:\n",
        "            rand_idx = random.randrange(0,len(self.backgrounds))\n",
        "\n",
        "            # Some backgrounds are RGB\n",
        "            img = self.backgrounds[rand_idx]\n",
        "            car = set_background(car, labels, img)\n",
        "\n",
        "        car = np.moveaxis(car, 2, 0)\n",
        "\n",
        "        return car, labels/10"
      ],
      "metadata": {
        "id": "-CwLF9TZCOJL"
      },
      "id": "-CwLF9TZCOJL",
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader, Subset\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "black_car = []\n",
        "orange_car = []\n",
        "photos = []\n",
        "for file in os.listdir(f'{drive_path}carseg_data/arrays'):\n",
        "    if 'orange' in file: orange_car.append(file)\n",
        "    elif 'black' in file: black_car.append(file)\n",
        "    elif 'photo' in file and '(' not in file: photos.append(file)\n",
        "\n",
        "root = f'{drive_path}carseg_data/arrays'\n",
        "\n",
        "\n",
        "photo_test = photos[:30]\n",
        "photos = photos[30:]\n",
        "\n",
        "black_train, _ = train_test_split(black_car, test_size=0.1, random_state=42, shuffle=True)\n",
        "orange_train, _ = train_test_split(orange_car, test_size=0.1, random_state=42, shuffle=True)\n",
        "photos_train, photos_val = train_test_split(photos, test_size=0.2, random_state=42, shuffle=True)\n",
        "\n",
        "\n",
        "photos_train_ds = CarDataset(root, photos_train*3+black_train[200:400]+orange_train[200:400])\n",
        "train1_ds = CarDataset(root, photos_train*2, rotate_car=True, move_car=True, backgrounds=background_list)\n",
        "train2_ds = CarDataset(root, black_train[:200]+orange_train[:200], backgrounds=background_list)\n",
        "\n",
        "val_ds = CarDataset(root, photos_val)\n",
        "test_ds = CarDataset(root, photo_test)\n",
        "\n",
        "\n",
        "train_loader = DataLoader(photos_train_ds+train1_ds+train2_ds, batch_size=16, shuffle=True)\n",
        "val_loader = DataLoader(val_ds, batch_size=16)\n",
        "test_loader =  DataLoader(test_ds, batch_size=16)\n"
      ],
      "metadata": {
        "id": "zRX1q0M_RY9R"
      },
      "id": "zRX1q0M_RY9R",
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ConvBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super().__init__()\n",
        "        self.block = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout2d(0.3),\n",
        "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout2d(0.3)\n",
        "        )\n",
        "\n",
        "        for layer in self.block:\n",
        "            if isinstance(layer, nn.Conv2d):\n",
        "                init.kaiming_normal_(layer.weight, mode='fan_out', nonlinearity='relu')\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.block(x)\n",
        "\n",
        "class UNet(nn.Module):\n",
        "    def __init__(self, in_channels, num_classes):\n",
        "        super(UNet, self).__init__()\n",
        "        self.encoder0 = nn.Sequential(ConvBlock(in_channels, 64))\n",
        "        self.encoder1 = nn.Sequential(nn.MaxPool2d(2,2), ConvBlock(64, 128))\n",
        "        self.encoder2 = nn.Sequential(nn.MaxPool2d(2,2), ConvBlock(128, 256))\n",
        "        self.encoder3 = nn.Sequential(nn.MaxPool2d(2,2), ConvBlock(256, 512))\n",
        "        self.bottleneck = nn.Sequential(nn.MaxPool2d(2,2), ConvBlock(512,1024), nn.ConvTranspose2d(1024, 512, kernel_size=2, stride=2))\n",
        "        self.decoder0 = nn.Sequential(ConvBlock(1024,512), nn.ConvTranspose2d(512, 256, kernel_size=2, stride=2))\n",
        "        self.decoder1 = nn.Sequential(ConvBlock(512,256), nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2))\n",
        "        self.decoder2 = nn.Sequential(ConvBlock(256,128), nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2))\n",
        "        self.decoder3 = nn.Sequential(ConvBlock(128,64), nn.Conv2d(in_channels=64, out_channels=num_classes, kernel_size=1))\n",
        "\n",
        "    def forward(self, x):\n",
        "        x0 = self.encoder0(x)\n",
        "        x1 = self.encoder1(x0)\n",
        "        x2 = self.encoder2(x1)\n",
        "        x3 = self.encoder3(x2)\n",
        "        x4 = self.bottleneck(x3)\n",
        "        x4 = self.decoder0(torch.cat([x3,x4],dim=1))\n",
        "        x4 = self.decoder1(torch.cat([x2,x4],dim=1))\n",
        "        x4 = self.decoder2(torch.cat([x1,x4],dim=1))\n",
        "        x4 = self.decoder3(torch.cat([x0,x4],dim=1))\n",
        "\n",
        "        return x4"
      ],
      "metadata": {
        "id": "1kNd_9agvsB5"
      },
      "id": "1kNd_9agvsB5",
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\"\n",
        "model = UNet(3, 10).to(device)"
      ],
      "metadata": {
        "id": "mTOh4IglxbHw"
      },
      "id": "mTOh4IglxbHw",
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)"
      ],
      "metadata": {
        "id": "PeWijAip7nDh"
      },
      "id": "PeWijAip7nDh",
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 200\n",
        "dice = Dice(average='micro')\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    total_train_loss = 0.0\n",
        "    dice_scores_train = []\n",
        "\n",
        "\n",
        "    for batch in train_loader:\n",
        "        inputs, labels = batch\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        inputs = inputs.float()\n",
        "        labels = labels.long().to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(inputs)\n",
        "\n",
        "        # Calculate loss\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        total_train_loss += loss.item()\n",
        "\n",
        "        # Calculate dice\n",
        "        _, pred = torch.max(outputs, 1)\n",
        "        pred_cpu = pred.to('cpu')\n",
        "        labels_cpu = labels.to('cpu')\n",
        "\n",
        "        dice_scores_train.append(dice(pred_cpu, labels_cpu))\n",
        "\n",
        "        # Backpropagation and optimization\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    # Validation loop\n",
        "    model.eval()\n",
        "    total_val_loss = 0.0\n",
        "    dice_scores_val = []\n",
        "\n",
        "    for batch in val_loader:\n",
        "        inputs, labels = batch\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        inputs = inputs.float()\n",
        "        labels = labels.long().to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model(inputs)\n",
        "            val_loss = criterion(outputs, labels)\n",
        "            total_val_loss += val_loss.item()\n",
        "\n",
        "            # Calculate dice\n",
        "            _, pred = torch.max(outputs, 1)\n",
        "            pred_cpu = pred.to('cpu')\n",
        "            labels_cpu = labels.to('cpu')\n",
        "\n",
        "            dice_scores_val.append(dice(pred_cpu, labels_cpu))\n",
        "\n",
        "\n",
        "    print(f\"Epoch {epoch + 1}/{num_epochs}, Train Loss: {(total_train_loss / len(train_loader)):.4f}, Train dice: {np.mean(dice_scores_train):.4f}, Val Loss: {(total_val_loss / len(val_loader)):.4f}, Val dice: {np.mean(dice_scores_val):.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ugfQCb45aC-",
        "outputId": "31928000-a955-4cd3-8712-25f13b5fca98"
      },
      "id": "8ugfQCb45aC-",
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200, Train Loss: 1.3272, Train dice: 0.6424, Val Loss: 0.9920, Val dice: 0.7149\n",
            "Epoch 2/200, Train Loss: 0.8767, Train dice: 0.7326, Val Loss: 0.9901, Val dice: 0.7018\n",
            "Epoch 3/200, Train Loss: 0.8196, Train dice: 0.7451, Val Loss: 0.9673, Val dice: 0.7068\n",
            "Epoch 4/200, Train Loss: 0.7753, Train dice: 0.7591, Val Loss: 0.9442, Val dice: 0.7157\n",
            "Epoch 5/200, Train Loss: 0.7512, Train dice: 0.7623, Val Loss: 0.9702, Val dice: 0.7038\n",
            "Epoch 6/200, Train Loss: 0.7284, Train dice: 0.7676, Val Loss: 0.8733, Val dice: 0.7296\n",
            "Epoch 7/200, Train Loss: 0.7094, Train dice: 0.7734, Val Loss: 0.8391, Val dice: 0.7393\n",
            "Epoch 8/200, Train Loss: 0.6920, Train dice: 0.7773, Val Loss: 0.8592, Val dice: 0.7378\n",
            "Epoch 9/200, Train Loss: 0.7047, Train dice: 0.7762, Val Loss: 0.8367, Val dice: 0.7502\n",
            "Epoch 10/200, Train Loss: 0.6815, Train dice: 0.7792, Val Loss: 0.8250, Val dice: 0.7497\n",
            "Epoch 11/200, Train Loss: 0.6726, Train dice: 0.7829, Val Loss: 0.8206, Val dice: 0.7535\n",
            "Epoch 12/200, Train Loss: 0.6662, Train dice: 0.7828, Val Loss: 0.7977, Val dice: 0.7511\n",
            "Epoch 13/200, Train Loss: 0.6546, Train dice: 0.7884, Val Loss: 0.8175, Val dice: 0.7464\n",
            "Epoch 14/200, Train Loss: 0.6526, Train dice: 0.7890, Val Loss: 0.7887, Val dice: 0.7592\n",
            "Epoch 15/200, Train Loss: 0.6395, Train dice: 0.7904, Val Loss: 0.7483, Val dice: 0.7738\n",
            "Epoch 16/200, Train Loss: 0.6186, Train dice: 0.7958, Val Loss: 0.7928, Val dice: 0.7696\n",
            "Epoch 17/200, Train Loss: 0.6115, Train dice: 0.8011, Val Loss: 0.7525, Val dice: 0.7744\n",
            "Epoch 18/200, Train Loss: 0.5974, Train dice: 0.8040, Val Loss: 0.7433, Val dice: 0.7758\n",
            "Epoch 19/200, Train Loss: 0.5881, Train dice: 0.8071, Val Loss: 0.7132, Val dice: 0.7821\n",
            "Epoch 20/200, Train Loss: 0.5888, Train dice: 0.8081, Val Loss: 0.7609, Val dice: 0.7674\n",
            "Epoch 21/200, Train Loss: 0.5682, Train dice: 0.8135, Val Loss: 0.7139, Val dice: 0.7620\n",
            "Epoch 22/200, Train Loss: 0.5538, Train dice: 0.8174, Val Loss: 0.7663, Val dice: 0.7663\n",
            "Epoch 23/200, Train Loss: 0.5529, Train dice: 0.8186, Val Loss: 0.7195, Val dice: 0.7887\n",
            "Epoch 24/200, Train Loss: 0.5346, Train dice: 0.8242, Val Loss: 0.6825, Val dice: 0.7850\n",
            "Epoch 25/200, Train Loss: 0.5282, Train dice: 0.8269, Val Loss: 0.6670, Val dice: 0.7857\n",
            "Epoch 26/200, Train Loss: 0.5176, Train dice: 0.8306, Val Loss: 0.6320, Val dice: 0.8009\n",
            "Epoch 27/200, Train Loss: 0.5036, Train dice: 0.8346, Val Loss: 0.6442, Val dice: 0.7972\n",
            "Epoch 28/200, Train Loss: 0.4948, Train dice: 0.8372, Val Loss: 0.6474, Val dice: 0.7960\n",
            "Epoch 29/200, Train Loss: 0.5075, Train dice: 0.8341, Val Loss: 0.6856, Val dice: 0.7801\n",
            "Epoch 30/200, Train Loss: 0.4747, Train dice: 0.8441, Val Loss: 0.5993, Val dice: 0.8093\n",
            "Epoch 31/200, Train Loss: 0.4676, Train dice: 0.8457, Val Loss: 0.6268, Val dice: 0.8030\n",
            "Epoch 32/200, Train Loss: 0.4574, Train dice: 0.8477, Val Loss: 0.6334, Val dice: 0.8108\n",
            "Epoch 33/200, Train Loss: 0.4392, Train dice: 0.8543, Val Loss: 0.5808, Val dice: 0.8072\n",
            "Epoch 34/200, Train Loss: 0.4378, Train dice: 0.8546, Val Loss: 0.5811, Val dice: 0.8116\n",
            "Epoch 35/200, Train Loss: 0.4330, Train dice: 0.8555, Val Loss: 0.5772, Val dice: 0.8112\n",
            "Epoch 36/200, Train Loss: 0.4365, Train dice: 0.8553, Val Loss: 0.5607, Val dice: 0.8202\n",
            "Epoch 37/200, Train Loss: 0.4201, Train dice: 0.8603, Val Loss: 0.5318, Val dice: 0.8275\n",
            "Epoch 38/200, Train Loss: 0.4204, Train dice: 0.8601, Val Loss: 0.5526, Val dice: 0.8226\n",
            "Epoch 39/200, Train Loss: 0.4088, Train dice: 0.8630, Val Loss: 0.5640, Val dice: 0.8213\n",
            "Epoch 40/200, Train Loss: 0.4081, Train dice: 0.8646, Val Loss: 0.5254, Val dice: 0.8305\n",
            "Epoch 41/200, Train Loss: 0.3936, Train dice: 0.8682, Val Loss: 0.5090, Val dice: 0.8333\n",
            "Epoch 42/200, Train Loss: 0.3926, Train dice: 0.8698, Val Loss: 0.5160, Val dice: 0.8305\n",
            "Epoch 43/200, Train Loss: 0.3831, Train dice: 0.8723, Val Loss: 0.5000, Val dice: 0.8370\n",
            "Epoch 44/200, Train Loss: 0.3760, Train dice: 0.8746, Val Loss: 0.5034, Val dice: 0.8312\n",
            "Epoch 45/200, Train Loss: 0.3560, Train dice: 0.8794, Val Loss: 0.4831, Val dice: 0.8405\n",
            "Epoch 46/200, Train Loss: 0.3552, Train dice: 0.8802, Val Loss: 0.5215, Val dice: 0.8323\n",
            "Epoch 47/200, Train Loss: 0.3705, Train dice: 0.8754, Val Loss: 0.5107, Val dice: 0.8354\n",
            "Epoch 48/200, Train Loss: 0.3567, Train dice: 0.8790, Val Loss: 0.4800, Val dice: 0.8412\n",
            "Epoch 49/200, Train Loss: 0.3488, Train dice: 0.8805, Val Loss: 0.5089, Val dice: 0.8382\n",
            "Epoch 50/200, Train Loss: 0.3411, Train dice: 0.8839, Val Loss: 0.4891, Val dice: 0.8329\n",
            "Epoch 51/200, Train Loss: 0.3472, Train dice: 0.8820, Val Loss: 0.4815, Val dice: 0.8399\n",
            "Epoch 52/200, Train Loss: 0.3324, Train dice: 0.8862, Val Loss: 0.4862, Val dice: 0.8473\n",
            "Epoch 53/200, Train Loss: 0.3258, Train dice: 0.8879, Val Loss: 0.4832, Val dice: 0.8433\n",
            "Epoch 54/200, Train Loss: 0.3264, Train dice: 0.8887, Val Loss: 0.4758, Val dice: 0.8408\n",
            "Epoch 55/200, Train Loss: 0.3191, Train dice: 0.8904, Val Loss: 0.4882, Val dice: 0.8438\n",
            "Epoch 56/200, Train Loss: 0.3098, Train dice: 0.8932, Val Loss: 0.5005, Val dice: 0.8434\n",
            "Epoch 57/200, Train Loss: 0.3165, Train dice: 0.8930, Val Loss: 0.4904, Val dice: 0.8395\n",
            "Epoch 58/200, Train Loss: 0.3150, Train dice: 0.8924, Val Loss: 0.4564, Val dice: 0.8470\n",
            "Epoch 59/200, Train Loss: 0.3000, Train dice: 0.8962, Val Loss: 0.4490, Val dice: 0.8465\n",
            "Epoch 60/200, Train Loss: 0.3043, Train dice: 0.8962, Val Loss: 0.4988, Val dice: 0.8332\n",
            "Epoch 61/200, Train Loss: 0.3042, Train dice: 0.8958, Val Loss: 0.4918, Val dice: 0.8375\n",
            "Epoch 62/200, Train Loss: 0.2866, Train dice: 0.9006, Val Loss: 0.4514, Val dice: 0.8513\n",
            "Epoch 63/200, Train Loss: 0.3052, Train dice: 0.8938, Val Loss: 0.5119, Val dice: 0.8454\n",
            "Epoch 64/200, Train Loss: 0.2925, Train dice: 0.8990, Val Loss: 0.4962, Val dice: 0.8350\n",
            "Epoch 65/200, Train Loss: 0.2763, Train dice: 0.9031, Val Loss: 0.4617, Val dice: 0.8497\n",
            "Epoch 66/200, Train Loss: 0.2812, Train dice: 0.9024, Val Loss: 0.4584, Val dice: 0.8508\n",
            "Epoch 67/200, Train Loss: 0.2792, Train dice: 0.9024, Val Loss: 0.4587, Val dice: 0.8517\n",
            "Epoch 68/200, Train Loss: 0.2705, Train dice: 0.9059, Val Loss: 0.4623, Val dice: 0.8516\n",
            "Epoch 69/200, Train Loss: 0.2708, Train dice: 0.9063, Val Loss: 0.4270, Val dice: 0.8605\n",
            "Epoch 70/200, Train Loss: 0.2616, Train dice: 0.9086, Val Loss: 0.4953, Val dice: 0.8486\n",
            "Epoch 71/200, Train Loss: 0.2715, Train dice: 0.9056, Val Loss: 0.4634, Val dice: 0.8497\n",
            "Epoch 72/200, Train Loss: 0.2687, Train dice: 0.9054, Val Loss: 0.4603, Val dice: 0.8500\n",
            "Epoch 73/200, Train Loss: 0.2533, Train dice: 0.9116, Val Loss: 0.4477, Val dice: 0.8548\n",
            "Epoch 74/200, Train Loss: 0.2610, Train dice: 0.9092, Val Loss: 0.4444, Val dice: 0.8515\n",
            "Epoch 75/200, Train Loss: 0.2588, Train dice: 0.9104, Val Loss: 0.4538, Val dice: 0.8552\n",
            "Epoch 76/200, Train Loss: 0.2448, Train dice: 0.9139, Val Loss: 0.4481, Val dice: 0.8603\n",
            "Epoch 77/200, Train Loss: 0.2505, Train dice: 0.9133, Val Loss: 0.4594, Val dice: 0.8525\n",
            "Epoch 78/200, Train Loss: 0.2444, Train dice: 0.9151, Val Loss: 0.4474, Val dice: 0.8587\n",
            "Epoch 79/200, Train Loss: 0.2452, Train dice: 0.9145, Val Loss: 0.4433, Val dice: 0.8587\n",
            "Epoch 80/200, Train Loss: 0.2335, Train dice: 0.9186, Val Loss: 0.4296, Val dice: 0.8658\n",
            "Epoch 81/200, Train Loss: 0.2385, Train dice: 0.9170, Val Loss: 0.4447, Val dice: 0.8548\n",
            "Epoch 82/200, Train Loss: 0.2333, Train dice: 0.9202, Val Loss: 0.4708, Val dice: 0.8602\n",
            "Epoch 83/200, Train Loss: 0.2361, Train dice: 0.9192, Val Loss: 0.4213, Val dice: 0.8665\n",
            "Epoch 84/200, Train Loss: 0.2355, Train dice: 0.9206, Val Loss: 0.4357, Val dice: 0.8568\n",
            "Epoch 85/200, Train Loss: 0.2423, Train dice: 0.9172, Val Loss: 0.4745, Val dice: 0.8607\n",
            "Epoch 86/200, Train Loss: 0.2232, Train dice: 0.9235, Val Loss: 0.4124, Val dice: 0.8610\n",
            "Epoch 87/200, Train Loss: 0.2244, Train dice: 0.9241, Val Loss: 0.4553, Val dice: 0.8667\n",
            "Epoch 88/200, Train Loss: 0.2111, Train dice: 0.9272, Val Loss: 0.4731, Val dice: 0.8642\n",
            "Epoch 89/200, Train Loss: 0.2111, Train dice: 0.9278, Val Loss: 0.4968, Val dice: 0.8596\n",
            "Epoch 90/200, Train Loss: 0.2167, Train dice: 0.9262, Val Loss: 0.4157, Val dice: 0.8713\n",
            "Epoch 91/200, Train Loss: 0.2177, Train dice: 0.9258, Val Loss: 0.4063, Val dice: 0.8738\n",
            "Epoch 92/200, Train Loss: 0.2090, Train dice: 0.9285, Val Loss: 0.4182, Val dice: 0.8737\n",
            "Epoch 93/200, Train Loss: 0.2042, Train dice: 0.9307, Val Loss: 0.5002, Val dice: 0.8634\n",
            "Epoch 94/200, Train Loss: 0.1991, Train dice: 0.9333, Val Loss: 0.4592, Val dice: 0.8654\n",
            "Epoch 95/200, Train Loss: 0.2013, Train dice: 0.9322, Val Loss: 0.4103, Val dice: 0.8805\n",
            "Epoch 96/200, Train Loss: 0.1990, Train dice: 0.9326, Val Loss: 0.4145, Val dice: 0.8770\n",
            "Epoch 97/200, Train Loss: 0.1986, Train dice: 0.9334, Val Loss: 0.4608, Val dice: 0.8671\n",
            "Epoch 98/200, Train Loss: 0.1907, Train dice: 0.9369, Val Loss: 0.4286, Val dice: 0.8712\n",
            "Epoch 99/200, Train Loss: 0.1933, Train dice: 0.9349, Val Loss: 0.4053, Val dice: 0.8754\n",
            "Epoch 100/200, Train Loss: 0.1895, Train dice: 0.9368, Val Loss: 0.4412, Val dice: 0.8740\n",
            "Epoch 101/200, Train Loss: 0.1813, Train dice: 0.9395, Val Loss: 0.4395, Val dice: 0.8751\n",
            "Epoch 102/200, Train Loss: 0.1800, Train dice: 0.9398, Val Loss: 0.4407, Val dice: 0.8675\n",
            "Epoch 103/200, Train Loss: 0.1852, Train dice: 0.9378, Val Loss: 0.4268, Val dice: 0.8754\n",
            "Epoch 104/200, Train Loss: 0.1795, Train dice: 0.9399, Val Loss: 0.4665, Val dice: 0.8702\n",
            "Epoch 105/200, Train Loss: 0.1708, Train dice: 0.9426, Val Loss: 0.4592, Val dice: 0.8746\n",
            "Epoch 106/200, Train Loss: 0.1796, Train dice: 0.9403, Val Loss: 0.4505, Val dice: 0.8727\n",
            "Epoch 107/200, Train Loss: 0.1758, Train dice: 0.9405, Val Loss: 0.4442, Val dice: 0.8694\n",
            "Epoch 108/200, Train Loss: 0.1778, Train dice: 0.9407, Val Loss: 0.4854, Val dice: 0.8701\n",
            "Epoch 109/200, Train Loss: 0.1750, Train dice: 0.9412, Val Loss: 0.4461, Val dice: 0.8709\n",
            "Epoch 110/200, Train Loss: 0.1796, Train dice: 0.9402, Val Loss: 0.4159, Val dice: 0.8756\n",
            "Epoch 111/200, Train Loss: 0.1733, Train dice: 0.9417, Val Loss: 0.5829, Val dice: 0.8627\n",
            "Epoch 112/200, Train Loss: 0.1798, Train dice: 0.9402, Val Loss: 0.4167, Val dice: 0.8795\n",
            "Epoch 113/200, Train Loss: 0.1726, Train dice: 0.9423, Val Loss: 0.4131, Val dice: 0.8807\n",
            "Epoch 114/200, Train Loss: 0.1665, Train dice: 0.9441, Val Loss: 0.4135, Val dice: 0.8806\n",
            "Epoch 115/200, Train Loss: 0.1647, Train dice: 0.9443, Val Loss: 0.4361, Val dice: 0.8790\n",
            "Epoch 116/200, Train Loss: 0.1790, Train dice: 0.9412, Val Loss: 0.4218, Val dice: 0.8796\n",
            "Epoch 117/200, Train Loss: 0.1635, Train dice: 0.9446, Val Loss: 0.4650, Val dice: 0.8764\n",
            "Epoch 118/200, Train Loss: 0.1547, Train dice: 0.9474, Val Loss: 0.4297, Val dice: 0.8793\n",
            "Epoch 119/200, Train Loss: 0.1586, Train dice: 0.9458, Val Loss: 0.4567, Val dice: 0.8709\n",
            "Epoch 120/200, Train Loss: 0.1531, Train dice: 0.9482, Val Loss: 0.4325, Val dice: 0.8796\n",
            "Epoch 121/200, Train Loss: 0.1531, Train dice: 0.9477, Val Loss: 0.5469, Val dice: 0.8647\n",
            "Epoch 122/200, Train Loss: 0.1653, Train dice: 0.9447, Val Loss: 0.4667, Val dice: 0.8790\n",
            "Epoch 123/200, Train Loss: 0.1530, Train dice: 0.9477, Val Loss: 0.4380, Val dice: 0.8805\n",
            "Epoch 124/200, Train Loss: 0.1582, Train dice: 0.9463, Val Loss: 0.4472, Val dice: 0.8784\n",
            "Epoch 125/200, Train Loss: 0.1610, Train dice: 0.9464, Val Loss: 0.4512, Val dice: 0.8759\n",
            "Epoch 126/200, Train Loss: 0.1604, Train dice: 0.9460, Val Loss: 0.4806, Val dice: 0.8699\n",
            "Epoch 127/200, Train Loss: 0.1582, Train dice: 0.9464, Val Loss: 0.5021, Val dice: 0.8716\n",
            "Epoch 128/200, Train Loss: 0.1558, Train dice: 0.9476, Val Loss: 0.4868, Val dice: 0.8740\n",
            "Epoch 129/200, Train Loss: 0.1495, Train dice: 0.9491, Val Loss: 0.4576, Val dice: 0.8792\n",
            "Epoch 130/200, Train Loss: 0.1553, Train dice: 0.9478, Val Loss: 0.4254, Val dice: 0.8787\n",
            "Epoch 131/200, Train Loss: 0.1525, Train dice: 0.9484, Val Loss: 0.4465, Val dice: 0.8767\n",
            "Epoch 132/200, Train Loss: 0.1491, Train dice: 0.9495, Val Loss: 0.4839, Val dice: 0.8740\n",
            "Epoch 133/200, Train Loss: 0.1641, Train dice: 0.9453, Val Loss: 0.4377, Val dice: 0.8775\n",
            "Epoch 134/200, Train Loss: 0.1654, Train dice: 0.9449, Val Loss: 0.5131, Val dice: 0.8696\n",
            "Epoch 135/200, Train Loss: 0.1613, Train dice: 0.9455, Val Loss: 0.4998, Val dice: 0.8701\n",
            "Epoch 136/200, Train Loss: 0.1479, Train dice: 0.9499, Val Loss: 0.4368, Val dice: 0.8804\n",
            "Epoch 137/200, Train Loss: 0.1472, Train dice: 0.9503, Val Loss: 0.4535, Val dice: 0.8764\n",
            "Epoch 138/200, Train Loss: 0.1456, Train dice: 0.9506, Val Loss: 0.4687, Val dice: 0.8703\n",
            "Epoch 139/200, Train Loss: 0.1437, Train dice: 0.9510, Val Loss: 0.4399, Val dice: 0.8794\n",
            "Epoch 140/200, Train Loss: 0.1495, Train dice: 0.9497, Val Loss: 0.4826, Val dice: 0.8774\n",
            "Epoch 141/200, Train Loss: 0.1367, Train dice: 0.9529, Val Loss: 0.4181, Val dice: 0.8856\n",
            "Epoch 142/200, Train Loss: 0.1463, Train dice: 0.9502, Val Loss: 0.4845, Val dice: 0.8773\n",
            "Epoch 143/200, Train Loss: 0.1401, Train dice: 0.9523, Val Loss: 0.4883, Val dice: 0.8785\n",
            "Epoch 144/200, Train Loss: 0.1387, Train dice: 0.9524, Val Loss: 0.4891, Val dice: 0.8756\n",
            "Epoch 145/200, Train Loss: 0.1451, Train dice: 0.9507, Val Loss: 0.4883, Val dice: 0.8758\n",
            "Epoch 146/200, Train Loss: 0.1448, Train dice: 0.9513, Val Loss: 0.4377, Val dice: 0.8803\n",
            "Epoch 147/200, Train Loss: 0.1429, Train dice: 0.9516, Val Loss: 0.4903, Val dice: 0.8764\n",
            "Epoch 148/200, Train Loss: 0.1372, Train dice: 0.9532, Val Loss: 0.4776, Val dice: 0.8737\n",
            "Epoch 149/200, Train Loss: 0.1375, Train dice: 0.9526, Val Loss: 0.4681, Val dice: 0.8782\n",
            "Epoch 150/200, Train Loss: 0.1449, Train dice: 0.9515, Val Loss: 0.5082, Val dice: 0.8732\n",
            "Epoch 151/200, Train Loss: 0.1421, Train dice: 0.9519, Val Loss: 0.4893, Val dice: 0.8747\n",
            "Epoch 152/200, Train Loss: 0.1479, Train dice: 0.9503, Val Loss: 0.4657, Val dice: 0.8809\n",
            "Epoch 153/200, Train Loss: 0.1381, Train dice: 0.9529, Val Loss: 0.4815, Val dice: 0.8791\n",
            "Epoch 154/200, Train Loss: 0.1482, Train dice: 0.9505, Val Loss: 0.4815, Val dice: 0.8758\n",
            "Epoch 155/200, Train Loss: 0.1399, Train dice: 0.9518, Val Loss: 0.4005, Val dice: 0.8842\n",
            "Epoch 156/200, Train Loss: 0.1331, Train dice: 0.9540, Val Loss: 0.4308, Val dice: 0.8843\n",
            "Epoch 157/200, Train Loss: 0.1429, Train dice: 0.9518, Val Loss: 0.5340, Val dice: 0.8716\n",
            "Epoch 158/200, Train Loss: 0.1357, Train dice: 0.9534, Val Loss: 0.5089, Val dice: 0.8708\n",
            "Epoch 159/200, Train Loss: 0.1353, Train dice: 0.9535, Val Loss: 0.4502, Val dice: 0.8835\n",
            "Epoch 160/200, Train Loss: 0.1304, Train dice: 0.9551, Val Loss: 0.4382, Val dice: 0.8793\n",
            "Epoch 161/200, Train Loss: 0.1325, Train dice: 0.9545, Val Loss: 0.4517, Val dice: 0.8863\n",
            "Epoch 162/200, Train Loss: 0.1344, Train dice: 0.9542, Val Loss: 0.5213, Val dice: 0.8696\n",
            "Epoch 163/200, Train Loss: 0.1474, Train dice: 0.9504, Val Loss: 0.4881, Val dice: 0.8722\n",
            "Epoch 164/200, Train Loss: 0.1335, Train dice: 0.9540, Val Loss: 0.5199, Val dice: 0.8701\n",
            "Epoch 165/200, Train Loss: 0.1252, Train dice: 0.9565, Val Loss: 0.4503, Val dice: 0.8851\n",
            "Epoch 166/200, Train Loss: 0.1253, Train dice: 0.9568, Val Loss: 0.5013, Val dice: 0.8690\n",
            "Epoch 167/200, Train Loss: 0.1337, Train dice: 0.9539, Val Loss: 0.4828, Val dice: 0.8767\n",
            "Epoch 168/200, Train Loss: 0.1304, Train dice: 0.9551, Val Loss: 0.4634, Val dice: 0.8799\n",
            "Epoch 169/200, Train Loss: 0.1209, Train dice: 0.9580, Val Loss: 0.4923, Val dice: 0.8740\n",
            "Epoch 170/200, Train Loss: 0.1228, Train dice: 0.9572, Val Loss: 0.4800, Val dice: 0.8823\n",
            "Epoch 171/200, Train Loss: 0.1209, Train dice: 0.9576, Val Loss: 0.5283, Val dice: 0.8767\n",
            "Epoch 172/200, Train Loss: 0.1481, Train dice: 0.9512, Val Loss: 0.4403, Val dice: 0.8801\n",
            "Epoch 173/200, Train Loss: 0.1429, Train dice: 0.9512, Val Loss: 0.4948, Val dice: 0.8761\n",
            "Epoch 174/200, Train Loss: 0.1444, Train dice: 0.9519, Val Loss: 0.5205, Val dice: 0.8728\n",
            "Epoch 175/200, Train Loss: 0.1551, Train dice: 0.9488, Val Loss: 0.4856, Val dice: 0.8748\n",
            "Epoch 176/200, Train Loss: 0.1412, Train dice: 0.9518, Val Loss: 0.5211, Val dice: 0.8711\n",
            "Epoch 177/200, Train Loss: 0.1301, Train dice: 0.9550, Val Loss: 0.4362, Val dice: 0.8818\n",
            "Epoch 178/200, Train Loss: 0.1257, Train dice: 0.9564, Val Loss: 0.5346, Val dice: 0.8769\n",
            "Epoch 179/200, Train Loss: 0.1200, Train dice: 0.9583, Val Loss: 0.4978, Val dice: 0.8809\n",
            "Epoch 180/200, Train Loss: 0.1190, Train dice: 0.9588, Val Loss: 0.5490, Val dice: 0.8726\n",
            "Epoch 181/200, Train Loss: 0.1264, Train dice: 0.9569, Val Loss: 0.5164, Val dice: 0.8758\n",
            "Epoch 182/200, Train Loss: 0.1279, Train dice: 0.9564, Val Loss: 0.4826, Val dice: 0.8732\n",
            "Epoch 183/200, Train Loss: 0.1376, Train dice: 0.9535, Val Loss: 0.5133, Val dice: 0.8658\n",
            "Epoch 184/200, Train Loss: 0.1343, Train dice: 0.9539, Val Loss: 0.4655, Val dice: 0.8810\n",
            "Epoch 185/200, Train Loss: 0.1315, Train dice: 0.9553, Val Loss: 0.4665, Val dice: 0.8803\n",
            "Epoch 186/200, Train Loss: 0.1283, Train dice: 0.9557, Val Loss: 0.5610, Val dice: 0.8673\n",
            "Epoch 187/200, Train Loss: 0.1234, Train dice: 0.9571, Val Loss: 0.5317, Val dice: 0.8699\n",
            "Epoch 188/200, Train Loss: 0.1206, Train dice: 0.9584, Val Loss: 0.4961, Val dice: 0.8836\n",
            "Epoch 189/200, Train Loss: 0.1258, Train dice: 0.9571, Val Loss: 0.5083, Val dice: 0.8763\n",
            "Epoch 190/200, Train Loss: 0.1222, Train dice: 0.9576, Val Loss: 0.5034, Val dice: 0.8804\n",
            "Epoch 191/200, Train Loss: 0.1142, Train dice: 0.9601, Val Loss: 0.5553, Val dice: 0.8735\n",
            "Epoch 192/200, Train Loss: 0.1199, Train dice: 0.9587, Val Loss: 0.5872, Val dice: 0.8718\n",
            "Epoch 193/200, Train Loss: 0.1158, Train dice: 0.9596, Val Loss: 0.5556, Val dice: 0.8707\n",
            "Epoch 194/200, Train Loss: 0.1319, Train dice: 0.9551, Val Loss: 0.4582, Val dice: 0.8823\n",
            "Epoch 195/200, Train Loss: 0.1242, Train dice: 0.9574, Val Loss: 0.5134, Val dice: 0.8740\n",
            "Epoch 196/200, Train Loss: 0.1232, Train dice: 0.9575, Val Loss: 0.5928, Val dice: 0.8672\n",
            "Epoch 197/200, Train Loss: 0.1402, Train dice: 0.9530, Val Loss: 0.5346, Val dice: 0.8709\n",
            "Epoch 198/200, Train Loss: 0.1374, Train dice: 0.9537, Val Loss: 0.5327, Val dice: 0.8713\n",
            "Epoch 199/200, Train Loss: 0.1265, Train dice: 0.9567, Val Loss: 0.4972, Val dice: 0.8761\n",
            "Epoch 200/200, Train Loss: 0.1206, Train dice: 0.9583, Val Loss: 0.4887, Val dice: 0.8813\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def test_model(model, loader):\n",
        "  \"\"\"Test a model on a test dataset\"\"\"\n",
        "  dice = Dice(average='micro')\n",
        "  model.eval()\n",
        "  dice_scores = []\n",
        "\n",
        "  for batch in loader:\n",
        "      inputs, labels = batch\n",
        "      inputs, labels = batch\n",
        "      inputs = inputs.to(device)\n",
        "      labels = labels.to(device)\n",
        "\n",
        "      inputs = inputs.float()\n",
        "      labels = labels.long().to(device)\n",
        "\n",
        "      with torch.no_grad():\n",
        "          outputs = model(inputs)\n",
        "\n",
        "          # Calculate accuracy on the test set\n",
        "          _, pred = torch.max(outputs, 1)\n",
        "\n",
        "          # Move tensors to CPU before performing numpy operations\n",
        "          pred_cpu = pred.to('cpu')\n",
        "          labels_cpu = labels.to('cpu')\n",
        "\n",
        "          dice_scores.append(dice(pred_cpu, labels_cpu))\n",
        "\n",
        "  return np.mean(dice_scores)\n",
        "\n"
      ],
      "metadata": {
        "id": "xQaY9sY_cI_V"
      },
      "id": "xQaY9sY_cI_V",
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_model(model, test_loader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XJ_nju9sx2RG",
        "outputId": "f86bb532-0354-48e5-b80b-9c0bc7d007a5"
      },
      "id": "XJ_nju9sx2RG",
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9012814"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model.state_dict(), f'{drive_path}carseg_data/model_90_dice.pth')"
      ],
      "metadata": {
        "id": "QV0-TVG82c47"
      },
      "id": "QV0-TVG82c47",
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hpNuJMT_KwAg"
      },
      "id": "hpNuJMT_KwAg",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.10"
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "V100"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}